{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NFRa4IjWixI6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "import numpy as np\n",
        "from skimage.io import imread  # reading images\n",
        "from skimage.transform import resize  # resizing images\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from coders import LabelEncoder, DecodePredictions\n",
        "from retinamodels import RetinaNet\n",
        "from losses import RetinaNetLoss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxsY-Ti4A_ZD",
        "outputId": "6d1d380d-de92-483b-a931-2b77b38f68ec"
      },
      "outputs": [],
      "source": [
        "url = \"http://agristats.eu/images.zip\"\n",
        "filename = os.path.join(os.getcwd(), \"images.zip\")\n",
        "keras.utils.get_file(filename, url)\n",
        "\n",
        "with zipfile.ZipFile(\"images.zip\", \"r\") as z_fp:\n",
        "    z_fp.extractall(\"./\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xWgxODNvixJR"
      },
      "outputs": [],
      "source": [
        "model_dir = \"retinanet/\"\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "num_classes = 80\n",
        "batch_size = 2\n",
        "\n",
        "learning_rates = [2.5e-06, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=learning_rate_boundaries, values=learning_rates\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4TwwSdqixJS",
        "outputId": "fc2a5d4e-8582-46b0-f13f-4c4800628526"
      },
      "outputs": [],
      "source": [
        "resnet50_backbone = get_backbone()\n",
        "loss_fn = RetinaNetLoss(num_classes)\n",
        "model = RetinaNet(num_classes, resnet50_backbone)\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
        "model.compile(loss=loss_fn, optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "YHKIAAlvixJT",
        "outputId": "d2ac10ee-f714-4579-f0db-f7df9aee2cdd"
      },
      "outputs": [],
      "source": [
        "callbacks_list = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=os.path.join(model_dir, \"weights\" + \"_epoch_{epoch}\"),\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=False,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "\n",
        "import json\n",
        "import io\n",
        "# url = \"http://agristats.eu/images.json\"\n",
        "# filename = os.path.join(os.getcwd(), \"images.json\")\n",
        "# keras.utils.get_file(filename, url)\n",
        "with open('images.json') as json_file:\n",
        "    train_data = json.load(json_file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "directory = os.fsencode(\"images\")\n",
        "for file in os.listdir(directory):\n",
        "  filename = os.fsdecode(file)\n",
        "  for item in train_data:\n",
        "    if filename == item[\"image/filename\"]: \n",
        "      image = load_img(os.path.join(\"./images/\", filename))\n",
        "      image = img_to_array(image)\n",
        "      item[\"image\"] = image\n",
        "      item[\"label\"] = random.randint(0,1)\n",
        "myimages = pd.DataFrame.from_dict(train_data).to_dict(\"list\")\n",
        "myimages = tf.data.Dataset.from_tensor_slices(myimages)\n",
        "\n",
        "crop_size = 300\n",
        "upscale_factor = 3\n",
        "input_size = crop_size // upscale_factor\n",
        "batch_size = 8\n",
        "#root_dir = '/content/images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "0s0pHzFpixJV"
      },
      "outputs": [],
      "source": [
        "train_dataset = myimages.take(180)\n",
        "val_dataset = myimages.skip(180) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "9d40tC1aixJW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "samplebbox Tensor(\"args_1:0\", shape=(4,), dtype=float32)\n",
            "bbox Tensor(\"stack:0\", shape=(1, 4), dtype=float32)\n",
            "ασδ Tensor(\"concat:0\", shape=(1, 4), dtype=float32) Tensor(\"mul_1:0\", shape=(2,), dtype=float32)\n",
            "1Number of train batches are:  tf.Tensor(180, shape=(), dtype=int64)\n",
            "2Number of train batches are:  tf.Tensor(180, shape=(), dtype=int64)\n",
            "3Number of train batches are:  tf.Tensor(180, shape=(), dtype=int64)\n"
          ]
        },
        {
          "ename": "NotImplementedError",
          "evalue": "in user code:\n\n    <ipython-input-21-db6f777045e5>:101 encode_batch  *\n        label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n    <ipython-input-21-db6f777045e5>:78 _encode_sample  *\n        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n    <ipython-input-20-4fc21d71ac79>:84 get_anchors  *\n        anchors = [\n    <ipython-input-20-4fc21d71ac79>:62 _get_anchors  *\n        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3387 meshgrid  **\n        mult_fact = ones(shapes, output_dtype)\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2967 ones\n        output = _constant_if_small(one, shape, dtype, name)\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2662 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:6 prod\n        \n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3052 prod\n        keepdims=keepdims, initial=initial, where=where)\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:749 __array__\n        \" array.\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (meshgrid/Size_1:0) to a numpy array.\n",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-35-cbea19a51755>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'3Number of train batches are: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m train_dataset = train_dataset.map(\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mlabel_encoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mautotune\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'4Number of train batches are: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcardinality\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic)\u001b[0m\n\u001b[0;32m   1626\u001b[0m           \u001b[0mnum_parallel_calls\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1627\u001b[0m           \u001b[0mdeterministic\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m           preserve_cardinality=True)\n\u001b[0m\u001b[0;32m   1629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[0;32m   4018\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4019\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4020\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   4021\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4022\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deterministic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"default\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3219\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3220\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3221\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3222\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3223\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2530\u001b[0m     \"\"\"\n\u001b[0;32m   2531\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 2532\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2533\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2534\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2494\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2495\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2496\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2497\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2498\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2775\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2777\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2779\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2667\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2669\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 981\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    983\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3212\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3213\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3214\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3215\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3154\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3156\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3157\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3158\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m           \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m           \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNotImplementedError\u001b[0m: in user code:\n\n    <ipython-input-21-db6f777045e5>:101 encode_batch  *\n        label = self._encode_sample(images_shape, gt_boxes[i], cls_ids[i])\n    <ipython-input-21-db6f777045e5>:78 _encode_sample  *\n        anchor_boxes = self._anchor_box.get_anchors(image_shape[1], image_shape[2])\n    <ipython-input-20-4fc21d71ac79>:84 get_anchors  *\n        anchors = [\n    <ipython-input-20-4fc21d71ac79>:62 _get_anchors  *\n        centers = tf.stack(tf.meshgrid(rx, ry), axis=-1) * self._strides[level - 3]\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:3387 meshgrid  **\n        mult_fact = ones(shapes, output_dtype)\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2967 ones\n        output = _constant_if_small(one, shape, dtype, name)\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py:2662 _constant_if_small\n        if np.prod(shape) < 1000:\n    <__array_function__ internals>:6 prod\n        \n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3052 prod\n        keepdims=keepdims, initial=initial, where=where)\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\numpy\\core\\fromnumeric.py:86 _wrapreduction\n        return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n    C:\\Users\\goofy\\anaconda3\\envs\\d2l\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:749 __array__\n        \" array.\".format(self.name))\n\n    NotImplementedError: Cannot convert a symbolic Tensor (meshgrid/Size_1:0) to a numpy array.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "autotune = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = train_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
        "print('1Number of train batches are: ', tf.data.experimental.cardinality(train_dataset))\n",
        "train_dataset = train_dataset.shuffle(8 * batch_size)\n",
        "print('2Number of train batches are: ', tf.data.experimental.cardinality(train_dataset))\n",
        "train_dataset = train_dataset.padded_batch(\n",
        "    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "print('3Number of train batches are: ', tf.data.experimental.cardinality(train_dataset))\n",
        "train_dataset = train_dataset.map(\n",
        "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
        ")\n",
        "print('4Number of train batches are: ', tf.data.experimental.cardinality(train_dataset))\n",
        "train_dataset = train_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "print('5Number of train batches are: ', tf.data.experimental.cardinality(train_dataset))\n",
        "train_dataset = train_dataset.prefetch(autotune)\n",
        "print('6Number of train batches are: ', tf.data.experimental.cardinality(train_dataset))\n",
        "\n",
        "print('1Number of validation batches are: ', tf.data.experimental.cardinality(val_dataset))\n",
        "val_dataset = val_dataset.map(preprocess_data, num_parallel_calls=autotune)\n",
        "print('2Number of validation batches are: ', tf.data.experimental.cardinality(val_dataset))\n",
        "val_dataset = val_dataset.padded_batch(\n",
        "    batch_size=1, padding_values=(0.0, 1e-8, -1), drop_remainder=True\n",
        ")\n",
        "print('3Number of validation batches are: ', tf.data.experimental.cardinality(val_dataset))\n",
        "val_dataset = val_dataset.map(label_encoder.encode_batch, num_parallel_calls=autotune)\n",
        "print('4Number of validation batches are: ', tf.data.experimental.cardinality(val_dataset))\n",
        "val_dataset = val_dataset.apply(tf.data.experimental.ignore_errors())\n",
        "print('5Number of validation batches are: ', tf.data.experimental.cardinality(val_dataset))\n",
        "val_dataset = val_dataset.prefetch(autotune)\n",
        "print('Final')\n",
        "print('Number of train batches are: are: ', tf.data.experimental.cardinality(val_dataset))\n",
        "print('Number of validation batches are: ', tf.data.experimental.cardinality(val_dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fIGzJ__ixJX"
      },
      "outputs": [],
      "source": [
        "# Uncomment the following lines, when training on full dataset\n",
        "# train_steps_per_epoch = dataset_info.splits[\"train\"].num_examples // batch_size\n",
        "# val_steps_per_epoch = \\\n",
        "#     dataset_info.splits[\"validation\"].num_examples // batch_size\n",
        "\n",
        "# train_steps = 4 * 100000\n",
        "# epochs = train_steps // train_steps_per_epoch\n",
        "\n",
        "epochs = 1\n",
        "\n",
        "# Running 100 training and 50 validation steps,\n",
        "# remove `.take` when training on the full dataset\n",
        "model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks_list,\n",
        "    verbose=1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdDB82RtixJZ"
      },
      "outputs": [],
      "source": [
        "# Change this to `model_dir` when not using the downloaded weights\n",
        "weights_dir = model_dir\n",
        "\n",
        "latest_checkpoint = tf.train.latest_checkpoint(weights_dir)\n",
        "model.load_weights(latest_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhPTsdN4ixJa"
      },
      "outputs": [],
      "source": [
        "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "predictions = model(image, training=False)\n",
        "detections = DecodePredictions(confidence_threshold=0.5)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs=image, outputs=detections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wryO8KvmixJh"
      },
      "outputs": [],
      "source": [
        "def prepare_image(image):\n",
        "    image, _, ratio = resize_and_pad_image(image, jitter=None)\n",
        "    image = tf.keras.applications.resnet.preprocess_input(image)\n",
        "    return tf.expand_dims(image, axis=0), ratio\n",
        "\n",
        "\n",
        "val_dataset = tfds.load(\"coco/2017\", split=\"validation\", data_dir=\"data\")\n",
        "int2str = dataset_info.features[\"label\"].int2str\n",
        "\n",
        "image = load_img('test1.jpg')\n",
        "image = img_to_array(image)\n",
        "\n",
        "input_image, ratio = prepare_image(image)\n",
        "detections = inference_model.predict(input_image)\n",
        "num_detections = detections.valid_detections[0]\n",
        "class_names = [\n",
        "    int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
        "]\n",
        "visualize_detections(\n",
        "    image,\n",
        "    detections.nmsed_boxes[0][:num_detections] / ratio,\n",
        "    class_names,\n",
        "    detections.nmsed_scores[0][:num_detections],\n",
        ")\n",
        "\n",
        "#for sample in val_dataset.take(4):\n",
        "#    print(sample[\"image\"])\n",
        "#    image = tf.cast(sample[\"image\"], dtype=tf.float32)\n",
        "#    input_image, ratio = prepare_image(image)\n",
        "#    detections = inference_model.predict(input_image)\n",
        "#    num_detections = detections.valid_detections[0]\n",
        "#    class_names = [\n",
        "#        int2str(int(x)) for x in detections.nmsed_classes[0][:num_detections]\n",
        "#    ]\n",
        "#    visualize_detections(\n",
        "#        image,\n",
        "#        detections.nmsed_boxes[0][:num_detections] / ratio,\n",
        "#        class_names,\n",
        "#        detections.nmsed_scores[0][:num_detections],\n",
        "#    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "objectDetection_SO.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
